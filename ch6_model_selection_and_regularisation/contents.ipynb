{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear Model Selection and Regularisation\n",
    "- how to improve a simple linear model\n",
    "- replacing LSF with alternatives\n",
    "- better prediction accuracy and interpretability\n",
    "\n",
    "- prediction acccuracy: when n >> p, the LSF model is good(low bias and variance), but when p>n, the LSF model has infinite fits, and performs badly on test data\n",
    "- shrinking or constraining coefficients, improvements can be made\n",
    "\n",
    "- model interpretability: methods can be used to automatically select features\n",
    "\n",
    "3 main ways\n",
    "- subset(selecting a subset)\n",
    "- shrinkage(coeffs are shrunken towards 0)\n",
    "- dimension reduction(computing linear combinations of variables)\n",
    "\n",
    "## 1. subset selection\n",
    "### 1.1 best subset\n",
    "- fit every combination of coefficients(pC0 to pCp)\n",
    "- choose the best combination based on some criteria\n",
    "- test error, Cp, AIC, BIC, R^2, or CV\n",
    "- RSS and R^2 increase monotonically, hence all variables will end up being selected.\n",
    "- however this causes high bias.\n",
    "- therefore use adjusted R^2 instead\n",
    "- slow\n",
    "\n",
    "### 1.2 stepwise\n",
    "- choose all p-k models that is M_k + 1 p\n",
    "- choose best M_k+1 model based on some criterion\n",
    "- select best model.\n",
    "more efficient\n",
    "- adds predictors each time.\n",
    "- adds best predictor each time.(greedy algorithm)\n",
    "\n",
    "\n",
    "#### backward stepwise\n",
    "- full set of predictors\n",
    "- consider all k models with all but one\n",
    "- select best model as M_k-1\n",
    "- select best model through M_0 to M_k-1\n",
    "- n has to be larger than p\n",
    "\n",
    "- hybrid selects forward stepwise, then eliminate through backwise\n",
    "- micmick closer the best selection\n",
    "\n",
    "\n",
    "### 1.3 optimal model\n",
    "- how to determine what to select\n",
    "- RSS and R^2 are unreliable due to their high training error bias\n",
    "- estimate test error through adjustment to training error to account for bias\n",
    "- estimate error by validation methods\n",
    "\n",
    "#### cp\n",
    "$C_p = \\frac{1}{n}(RSS + 2d\\hat\\sigma^2)$\n",
    "adds a penalty with larger predictors\n",
    "- $\\hat\\sigma$ is estimated from training set, the variance of error term(difference to the predicted value)\n",
    "- AIC is the same in terms of least sqaures\n",
    "- as in this case maximum likelihood == least squares\n",
    "\n",
    "- BIC replaces 2 with log(n), places larger penalty on more variables\n",
    "\n",
    "- adjusted R^2 = 1- R^2*(n-1)/(n-d-1), d is the number of predictors.\n",
    "- larger is better(lower test error)\n",
    "\n",
    "standard model rule:\n",
    "- when rankings for models are close\n",
    "- take the standard error for each model(may be found e.g. using bootstrap)\n",
    "- take all overlapping standard errors within 1 SE of the best performing model\n",
    "- select the model with least amount of predictors "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
